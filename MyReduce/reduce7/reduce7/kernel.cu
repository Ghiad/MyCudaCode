#include <bits/stdc++.h>
#include <cuda.h>
#include "cuda_runtime.h"
#include "device_launch_parameters.h"
#include <time.h>

#define THREAD_PER_BLOCK 256
#define WARP_SIZE 32

template <unsigned int blockSize>
__device__ __forceinline__ float warpReduceSum(float sum) {
	if (blockSize >= 32)sum += __shfl_down_sync(0xffffffff, sum, 16); // 0-16, 1-17, 2-18, etc.
	if (blockSize >= 16)sum += __shfl_down_sync(0xffffffff, sum, 8);// 0-8, 1-9, 2-10, etc.
	if (blockSize >= 8)sum += __shfl_down_sync(0xffffffff, sum, 4);// 0-4, 1-5, 2-6, etc.
	if (blockSize >= 4)sum += __shfl_down_sync(0xffffffff, sum, 2);// 0-2, 1-3, 4-6, 5-7, etc.
	if (blockSize >= 2)sum += __shfl_down_sync(0xffffffff, sum, 1);// 0-1, 2-3, 4-5, etc.
	return sum;
}

template <unsigned int blockSize, int NUM_PER_THREAD>
__global__ void reduce7(float* d_in, float* d_out, unsigned int n) {
	float sum = 0;

	// each thread loads one element from global to shared mem
	unsigned int tid = threadIdx.x;
	unsigned int i = blockIdx.x * (blockSize * NUM_PER_THREAD) + threadIdx.x;

#pragma unroll
	for (int iter = 0; iter < NUM_PER_THREAD; iter++) {
		sum += d_in[i + iter * blockSize];
	}

	// Shared mem for partial sums (one per warp in the block)
	static __shared__ float warpLevelSums[WARP_SIZE];
	const int laneId = threadIdx.x % WARP_SIZE;
	const int warpId = threadIdx.x / WARP_SIZE;

	sum = warpReduceSum<blockSize>(sum);

	if (laneId == 0)warpLevelSums[warpId] = sum;
	__syncthreads();
	// read from shared memory only if that warp existed
	sum = (threadIdx.x < blockDim.x / WARP_SIZE) ? warpLevelSums[laneId] : 0;
	// Final reduce using first warp
	if (warpId == 0) sum = warpReduceSum<blockSize / WARP_SIZE>(sum);
	// write result for this block to global mem
	if (tid == 0) d_out[blockIdx.x] = sum;
}


bool check(float* out, float* res, int n) {
	for (int i = 0; i < n; i++) {
		if (out[i] != res[i])
			return false;
	}
	return true;
}

int main() {
	const int N = 32 * 1024 * 1024;
	float* a = (float*)malloc(N * sizeof(float));
	float* d_a;
	cudaMalloc((void**)&d_a, N * sizeof(float));

	const int block_num = 1024;
	const int NUM_PER_BLOCK = N / block_num;
	const int NUM_PER_THREAD = NUM_PER_BLOCK / THREAD_PER_BLOCK;
	float* out = (float*)malloc(block_num * sizeof(float));
	float* d_out;
	cudaMalloc((void**)&d_out, block_num * sizeof(float));
	float* res = (float*)malloc(block_num * sizeof(float));

	for (int i = 0; i < N; i++) {
		a[i] = i % 456;
	}

	for (int i = 0; i < block_num; i++) {
		float cur = 0;
		for (int j = 0; j < NUM_PER_BLOCK; j++) {
			if (i * NUM_PER_BLOCK + j < N) {
				cur += a[i * NUM_PER_BLOCK + j];
			}
		}
		res[i] = cur;
	}

	cudaMemcpy(d_a, a, N * sizeof(float), cudaMemcpyHostToDevice);

	dim3 Grid(block_num, 1);
	dim3 Block(THREAD_PER_BLOCK, 1);

	cudaEvent_t start, stop;
	cudaEventCreate(&start);
	cudaEventCreate(&stop);
	float msecTotal = 0;
	int nIter = 1000;
	cudaEventRecord(start);
	for (int i = 0; i < nIter; i++) {
		reduce7<THREAD_PER_BLOCK, NUM_PER_THREAD> << <Grid, Block >> > (d_a, d_out, N);
	}
	cudaEventRecord(stop);
	cudaEventSynchronize(stop);
	cudaEventElapsedTime(&msecTotal, start, stop);

	cudaMemcpy(out, d_out, block_num * sizeof(float), cudaMemcpyDeviceToHost);

	if (check(out, res, block_num))printf("the ans is right, time : %f\n", msecTotal);
	else {
		printf("the ans is wrong\n");
		for (int i = 0; i < block_num; i++) {
			printf("%lf ", out[i]);
		}
		printf("\n");
	}

	cudaFree(d_a);
	cudaFree(d_out);
}